from fastapi import APIRouter, Depends, HTTPException, UploadFile, File, Form, Query
from sqlalchemy.orm import Session
from sqlalchemy import select
from typing import Dict, List, Optional, Any
import csv
import json
import io
import logging
from datetime import datetime
import httpx

from app.core.database import get_db
from app.models.invite_task import InviteTask
from app.models.invite_target import InviteTarget
from app.schemas.target import InviteTargetCreate
from app.core.auth import get_current_user_id

logger = logging.getLogger(__name__)
router = APIRouter()

@router.post("/tasks/{task_id}/import/file")
async def import_targets_from_file(
    task_id: int,
    file: UploadFile = File(...),
    source_name: str = Form("file_upload"),
    db: Session = Depends(get_db),
    user_id: int = Depends(get_current_user_id)
):
    """
    –ò–º–ø–æ—Ä—Ç —Ü–µ–ª–µ–≤–æ–π –∞—É–¥–∏—Ç–æ—Ä–∏–∏ –∏–∑ CSV/JSON —Ñ–∞–π–ª–∞
    """
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø –∫ –∑–∞–¥–∞—á–µ
    task = db.query(InviteTask).filter(
        InviteTask.id == task_id,
        InviteTask.user_id == user_id
    ).first()
    
    if not task:
        raise HTTPException(status_code=404, detail="Task not found")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø —Ñ–∞–π–ª–∞
    if not file.filename:
        raise HTTPException(status_code=400, detail="No filename provided")
    
    file_extension = file.filename.lower().split('.')[-1]
    if file_extension not in ['csv', 'json', 'txt']:
        raise HTTPException(status_code=400, detail="Unsupported file format. Use CSV, JSON or TXT")
    
    try:
        # –ß–∏—Ç–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞
        content = await file.read()
        content_str = content.decode('utf-8')
        
        # üîç –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: –ª–æ–≥–∏—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–º–ø–æ—Ä—Ç–∞
        logger.info(f"üîç DIAGNOSTIC: Starting file import for task {task_id}")
        logger.info(f"üîç DIAGNOSTIC: File: {file.filename}, size: {len(content)} bytes")
        logger.info(f"üîç DIAGNOSTIC: File extension: {file_extension}")
        logger.info(f"üîç DIAGNOSTIC: Task current target_count: {task.target_count}")
        
        imported_targets = []
        errors = []
        
        if file_extension == 'csv':
            imported_targets, errors = await _parse_csv_content(content_str)
        elif file_extension == 'json':
            imported_targets, errors = await _parse_json_content(content_str)
        elif file_extension == 'txt':
            imported_targets, errors = await _parse_txt_content(content_str)
        
        # üîç –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ñ–∞–π–ª–∞
        logger.info(f"üîç DIAGNOSTIC: Parsed file results - imported: {len(imported_targets)}, errors: {len(errors)}")
        
        if not imported_targets and errors:
            logger.error(f"üîç DIAGNOSTIC: File parsing failed with errors: {errors[:3]}")
            raise HTTPException(status_code=400, detail=f"Failed to parse file: {'; '.join(errors[:5])}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ü–µ–ª–∏ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
        saved_targets = []
        for i, target_data in enumerate(imported_targets):
            try:
                # üîç –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: –ª–æ–≥–∏—Ä—É–µ–º –∫–∞–∂–¥—É—é —Ü–µ–ª—å
                logger.debug(f"üîç DIAGNOSTIC: Processing target {i+1}: {target_data}")
                
                target = InviteTarget(
                    task_id=task_id,
                    username=target_data.get('username'),
                    phone_number=target_data.get('phone_number'),
                    user_id_platform=target_data.get('user_id_platform'),
                    email=target_data.get('email'),
                    full_name=target_data.get('full_name'),
                    source="file_import",
                    extra_data={
                        "source_file": file.filename,
                        "source_name": source_name,
                        "imported_at": datetime.utcnow().isoformat()
                    }
                )
                db.add(target)
                saved_targets.append(target)
                
                # üîç –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ–º —Å–æ–∑–¥–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞
                logger.debug(f"üîç DIAGNOSTIC: Created InviteTarget {i+1} for task {task_id}")
                
            except Exception as e:
                logger.error(f"üîç DIAGNOSTIC: Failed to create target {i+1}: {e}")
                errors.append(f"Failed to save target {target_data}: {str(e)}")
        
        # üîç –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: —Å–æ—Å—Ç–æ—è–Ω–∏–µ –ø–µ—Ä–µ–¥ –∫–æ–º–º–∏—Ç–æ–º
        logger.info(f"üîç DIAGNOSTIC: About to commit {len(saved_targets)} targets to database")
        
        # –°–Ω–∞—á–∞–ª–∞ –∫–æ–º–º–∏—Ç–∏–º –Ω–æ–≤—ã–µ —Ü–µ–ª–∏
        db.commit()
        
        # üîç –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: —Å–æ—Å—Ç–æ—è–Ω–∏–µ –ø–æ—Å–ª–µ –∫–æ–º–º–∏—Ç–∞
        logger.info(f"üîç DIAGNOSTIC: Committed {len(saved_targets)} targets successfully")
        
        # –ó–∞—Ç–µ–º –æ–±–Ω–æ–≤–ª—è–µ–º —Å—á–µ—Ç—á–∏–∫ —Ü–µ–ª–µ–π –≤ –∑–∞–¥–∞—á–µ (–ø–æ–ª—É—á–∞–µ–º —Ä–µ–∞–ª—å–Ω—ã–π count –∏–∑ –±–∞–∑—ã)
        count_query = select(InviteTarget).where(InviteTarget.task_id == task_id)
        count_result = db.execute(count_query)
        all_targets = count_result.scalars().all()
        
        # üîç –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: –ø–æ–¥—Å—á–µ—Ç —Ü–µ–ª–µ–π
        old_count = task.target_count
        new_count = len(all_targets)
        logger.info(f"üîç DIAGNOSTIC: Target count update - old: {old_count}, new: {new_count}")
        logger.info(f"üîç DIAGNOSTIC: Database query returned {len(all_targets)} targets for task {task_id}")
        
        task.target_count = new_count
        task.updated_at = datetime.utcnow()
        
        # üîç –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∑–∞–¥–∞—á–∏
        logger.info(f"üîç DIAGNOSTIC: Updating task.target_count from {old_count} to {new_count}")
        
        db.commit()
        
        # üîç –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: —Ñ–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
        logger.info(f"üîç DIAGNOSTIC: File import completed successfully")
        logger.info(f"üîç DIAGNOSTIC: Final task.target_count: {task.target_count}")
        
        logger.info(f"–ò–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ {len(saved_targets)} —Ü–µ–ª–µ–π –¥–ª—è –∑–∞–¥–∞—á–∏ {task_id} –∏–∑ —Ñ–∞–π–ª–∞ {file.filename}. –û–±—â–∏–π —Å—á–µ—Ç—á–∏–∫: {task.target_count}")
        
        return {
            "success": True,
            "imported_count": len(saved_targets),
            "error_count": len(errors),
            "total_processed": len(imported_targets) + len(errors),
            "file_name": file.filename,
            "source_name": source_name,
            "total_targets_in_task": task.target_count,  # –î–æ–±–∞–≤–ª—è–µ–º –æ–±—â–∏–π —Å—á–µ—Ç—á–∏–∫
            "errors": errors[:10] if errors else []  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 10 –æ—à–∏–±–æ–∫
        }
        
    except UnicodeDecodeError:
        raise HTTPException(status_code=400, detail="File encoding not supported. Use UTF-8")
    except Exception as e:
        db.rollback()
        logger.error(f"Error importing file {file.filename}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Import failed: {str(e)}")

@router.post("/tasks/{task_id}/import/parsing")
async def import_targets_from_parsing(
    task_id: int,
    request_data: dict,
    db: Session = Depends(get_db),
    user_id: int = Depends(get_current_user_id)
):
    """
    –ò–º–ø–æ—Ä—Ç —Ü–µ–ª–µ–≤–æ–π –∞—É–¥–∏—Ç–æ—Ä–∏–∏ –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ parsing-service
    """
    # ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–û: –∏–∑–≤–ª–µ–∫–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ body
    parsing_task_id = request_data.get("parsing_task_id")
    source_name = request_data.get("source_name", "parsing_import")
    limit = request_data.get("limit")
    
    if not parsing_task_id:
        raise HTTPException(status_code=400, detail="parsing_task_id is required")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø –∫ –∑–∞–¥–∞—á–µ
    task = db.query(InviteTask).filter(
        InviteTask.id == task_id,
        InviteTask.user_id == user_id
    ).first()
    
    if not task:
        raise HTTPException(status_code=404, detail="Task not found")
    
    try:
        # –ü–æ–ª—É—á–∞–µ–º JWT —Ç–æ–∫–µ–Ω –¥–ª—è –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ –≤ parsing-service
        token = await _get_jwt_token_for_parsing_service(user_id)
        
        # üîç –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: –ª–æ–≥–∏—Ä—É–µ–º –Ω–∞—á–∞–ª–æ –∏–º–ø–æ—Ä—Ç–∞ –∏–∑ –ø–∞—Ä—Å–∏–Ω–≥–∞
        logger.info(f"üîç DIAGNOSTIC: Starting parsing import for task {task_id}")
        logger.info(f"üîç DIAGNOSTIC: Parsing task ID: {parsing_task_id}")
        logger.info(f"üîç DIAGNOSTIC: User ID: {user_id}")
        logger.info(f"üîç DIAGNOSTIC: JWT token created for user_id: {user_id}")
        
        parsing_service_url = "http://parsing-service:8000"
        
        async with httpx.AsyncClient(timeout=30.0) as client:
            # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –∑–∞–¥–∞—á–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∏—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é
            task_response = await client.get(
                f"{parsing_service_url}/tasks/{parsing_task_id}",
                headers={"Authorization": f"Bearer {token}"}
            )
            
            if task_response.status_code == 404:
                raise HTTPException(status_code=404, detail="Parsing task not found")
            elif task_response.status_code != 200:
                raise HTTPException(status_code=500, detail="Failed to verify parsing task")
            
            task_data = task_response.json()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–Ω–æ—Å—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é
            if task_data.get('user_id') != user_id:
                raise HTTPException(status_code=404, detail="Parsing task not found")
            
            # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–∞—Ä—Å–∏–Ω–≥–∞
            results_params = {}
            if limit:
                results_params['limit'] = limit
            
            results_response = await client.get(
                f"{parsing_service_url}/results/{parsing_task_id}",
                headers={"Authorization": f"Bearer {token}"},
                params=results_params
            )
            
            if results_response.status_code != 200:
                raise HTTPException(status_code=500, detail="Failed to get parsing results")
            
            results_data = results_response.json()
            parsing_results = results_data.get('results', [])
            
            if not parsing_results:
                return {
                    "success": False,
                    "message": "No parsing results found for this task",
                    "parsing_task_id": parsing_task_id,
                    "imported_count": 0
                }
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤ —Ñ–æ—Ä–º–∞—Ç InviteTarget
            imported_targets = []
            errors = []
            
            for i, result in enumerate(parsing_results):
                try:
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞
                    target_data = {
                        "username": result.get('username', '').strip() or None,
                        "phone_number": result.get('author_phone', '').strip() or None,
                        "user_id_platform": result.get('platform_id', '').strip() or None,
                        "full_name": result.get('display_name', '').strip() or None,
                    }
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –µ—Å—Ç—å —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä
                    if not any([target_data["username"], target_data["phone_number"], 
                               target_data["user_id_platform"]]):
                        errors.append(f"Result {i}: No valid identifier found")
                        continue
                    
                    # –°–æ–∑–¥–∞–µ–º InviteTarget
                    invite_target = InviteTarget(
                        task_id=task_id,
                        username=target_data["username"],
                        phone_number=target_data["phone_number"],
                        user_id_platform=target_data["user_id_platform"],
                        full_name=target_data["full_name"],
                        source="parsing_import",
                        extra_data={
                            "parsing_task_id": parsing_task_id,
                            "parsing_result_id": result.get('id'),
                            "source_name": source_name,
                            "imported_at": datetime.utcnow().isoformat(),
                            "original_data": result  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                        }
                    )
                    
                    db.add(invite_target)
                    imported_targets.append(invite_target)
                    
                except Exception as e:
                    errors.append(f"Result {i}: {str(e)}")
                    logger.error(f"Error processing parsing result {i}: {e}")
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—á–µ—Ç—á–∏–∫ —Ü–µ–ª–µ–π –≤ –∑–∞–¥–∞—á–µ (–¥–æ–±–∞–≤–ª—è–µ–º –∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–º—É)
            current_count_query = select(InviteTarget).where(InviteTarget.task_id == task_id)
            current_count_result = db.execute(current_count_query)
            current_targets = current_count_result.scalars().all()
            
            # üîç –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: –ø–æ–¥—Å—á–µ—Ç —Ü–µ–ª–µ–π –ø—Ä–∏ –∏–º–ø–æ—Ä—Ç–µ –∏–∑ –ø–∞—Ä—Å–∏–Ω–≥–∞
            old_count = task.target_count
            targets_in_db = len(current_targets)
            new_targets_count = len(imported_targets)
            final_count = targets_in_db  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–∑ –ë–î
            
            logger.info(f"üîç DIAGNOSTIC: Parsing import count update")
            logger.info(f"üîç DIAGNOSTIC: Task {task_id} old target_count: {old_count}")
            logger.info(f"üîç DIAGNOSTIC: Current targets in DB: {targets_in_db}")
            logger.info(f"üîç DIAGNOSTIC: New targets imported: {new_targets_count}")
            logger.info(f"üîç DIAGNOSTIC: Final target_count will be: {final_count}")
            
            task.target_count = final_count
            task.updated_at = datetime.utcnow()
            
            db.commit()
            
            # üîç –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: —Ñ–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
            logger.info(f"üîç DIAGNOSTIC: Parsing import completed, task.target_count: {task.target_count}")
            
            logger.info(f"–ò–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ {len(imported_targets)} —Ü–µ–ª–µ–π –∏–∑ –∑–∞–¥–∞—á–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞ {parsing_task_id} –¥–ª—è –∑–∞–¥–∞—á–∏ {task_id}")
            
            return {
                "success": True,
                "message": f"Successfully imported {len(imported_targets)} targets from parsing results",
                "parsing_task_id": parsing_task_id,
                "task_id": task_id,
                "imported_count": len(imported_targets),
                "error_count": len(errors),
                "total_processed": len(parsing_results),
                "errors": errors[:10] if errors else [],  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 10 –æ—à–∏–±–æ–∫
                "source_name": source_name,
                "parsing_task_title": task_data.get('title', 'Unknown'),
                "parsing_platform": task_data.get('platform', 'telegram')
            }
        
    except HTTPException:
        raise
    except Exception as e:
        db.rollback()
        logger.error(f"Error importing from parsing-service: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Parsing import failed: {str(e)}")

@router.post("/import/validate")
async def validate_import_data(
    file: UploadFile = File(...),
    user_id: int = Depends(get_current_user_id)
):
    """
    –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –∏–º–ø–æ—Ä—Ç–∞ –±–µ–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
    """
    if not file.filename:
        raise HTTPException(status_code=400, detail="No filename provided")
    
    file_extension = file.filename.lower().split('.')[-1]
    if file_extension not in ['csv', 'json', 'txt']:
        raise HTTPException(status_code=400, detail="Unsupported file format")
    
    try:
        content = await file.read()
        content_str = content.decode('utf-8')
        
        if file_extension == 'csv':
            targets, errors = await _parse_csv_content(content_str)
        elif file_extension == 'json':
            targets, errors = await _parse_json_content(content_str)
        elif file_extension == 'txt':
            targets, errors = await _parse_txt_content(content_str)
        
        # –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö
        stats = {
            "total_records": len(targets) + len(errors),
            "valid_records": len(targets),
            "invalid_records": len(errors),
            "has_usernames": sum(1 for t in targets if t.get('username')),
            "has_phones": sum(1 for t in targets if t.get('phone_number')),
            "has_emails": sum(1 for t in targets if t.get('email')),
            "has_user_ids": sum(1 for t in targets if t.get('user_id_platform'))
        }
        
        return {
            "file_name": file.filename,
            "file_size": len(content),
            "validation_result": "valid" if targets and not errors else "invalid",
            "statistics": stats,
            "sample_records": targets[:5],  # –ü–µ—Ä–≤—ã–µ 5 –∑–∞–ø–∏—Å–µ–π –¥–ª—è –ø—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä–∞
            "errors": errors[:10] if errors else []
        }
        
    except UnicodeDecodeError:
        raise HTTPException(status_code=400, detail="File encoding not supported. Use UTF-8")
    except Exception as e:
        logger.error(f"Error validating file {file.filename}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Validation failed: {str(e)}")

# –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞

async def _parse_csv_content(content: str) -> tuple[List[Dict], List[str]]:
    """–ü–∞—Ä—Å–∏–Ω–≥ CSV —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ"""
    targets = []
    errors = []
    
    try:
        csv_reader = csv.DictReader(io.StringIO(content))
        
        for row_num, row in enumerate(csv_reader, start=2):  # –ù–∞—á–∏–Ω–∞–µ–º —Å 2 –∏–∑-–∑–∞ –∑–∞–≥–æ–ª–æ–≤–∫–∞
            try:
                target = {}
                
                # –ì–∏–±–∫–æ–µ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Å—Ç–æ–ª–±—Ü–æ–≤
                for key, value in row.items():
                    if not value or not value.strip():
                        continue
                        
                    key_lower = key.lower().strip()
                    value_clean = value.strip()
                    
                    if key_lower in ['username', 'user', 'login', 'nickname']:
                        target['username'] = value_clean
                    elif key_lower in ['phone', 'phone_number', 'telephone', 'mobile']:
                        target['phone_number'] = value_clean
                    elif key_lower in ['email', 'mail', 'email_address']:
                        target['email'] = value_clean
                    elif key_lower in ['user_id', 'id', 'user_id_platform', 'platform_id']:
                        target['user_id_platform'] = value_clean
                    elif key_lower in ['name', 'full_name', 'fullname', 'display_name']:
                        target['full_name'] = value_clean
                
                if target:  # –ï—Å–ª–∏ —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ –ø–æ–ª–µ –∑–∞–ø–æ–ª–Ω–µ–Ω–æ
                    targets.append(target)
                else:
                    errors.append(f"Row {row_num}: No valid data found")
                    
            except Exception as e:
                errors.append(f"Row {row_num}: {str(e)}")
                
    except Exception as e:
        errors.append(f"CSV parsing error: {str(e)}")
    
    return targets, errors

async def _parse_json_content(content: str) -> tuple[List[Dict], List[str]]:
    """–ü–∞—Ä—Å–∏–Ω–≥ JSON —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ"""
    targets = []
    errors = []
    
    try:
        data = json.loads(content)
        
        if isinstance(data, list):
            for i, item in enumerate(data):
                if isinstance(item, dict):
                    target = {}
                    if item.get('username'):
                        target['username'] = str(item['username']).strip()
                    if item.get('phone_number') or item.get('phone'):
                        target['phone_number'] = str(item.get('phone_number') or item.get('phone')).strip()
                    if item.get('email'):
                        target['email'] = str(item['email']).strip()
                    if item.get('user_id_platform') or item.get('user_id'):
                        target['user_id_platform'] = str(item.get('user_id_platform') or item.get('user_id')).strip()
                    if item.get('full_name') or item.get('name'):
                        target['full_name'] = str(item.get('full_name') or item.get('name')).strip()
                    
                    if target:
                        targets.append(target)
                    else:
                        errors.append(f"Item {i}: No valid data found")
                else:
                    errors.append(f"Item {i}: Expected object, got {type(item)}")
        else:
            errors.append("Expected JSON array")
            
    except json.JSONDecodeError as e:
        errors.append(f"JSON parsing error: {str(e)}")
    except Exception as e:
        errors.append(f"Unexpected error: {str(e)}")
    
    return targets, errors

async def _parse_txt_content(content: str) -> tuple[List[Dict], List[str]]:
    """–ü–∞—Ä—Å–∏–Ω–≥ TXT —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ (–æ–¥–Ω–æ –∑–Ω–∞—á–µ–Ω–∏–µ –Ω–∞ —Å—Ç—Ä–æ–∫—É)"""
    targets = []
    errors = []
    
    lines = content.strip().split('\n')
    
    for line_num, line in enumerate(lines, start=1):
        line = line.strip()
        if not line:
            continue
            
        target = {}
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö –ø–æ —Ñ–æ—Ä–º–∞—Ç—É
        if '@' in line and '.' in line:
            target['email'] = line
        elif line.startswith('+') or line.replace('-', '').replace(' ', '').isdigit():
            target['phone_number'] = line
        elif line.isdigit():
            target['user_id_platform'] = line
        else:
            target['username'] = line
        
        if target:
            targets.append(target)
        else:
            errors.append(f"Line {line_num}: Unrecognized format")
    
    return targets, errors

async def _get_jwt_token_for_parsing_service(user_id: int) -> str:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ JWT —Ç–æ–∫–µ–Ω–∞ –¥–ª—è –º–µ–∂—Å–µ—Ä–≤–∏—Å–Ω–æ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å Parsing Service"""
    try:
        from app.core.vault import get_vault_client
        from datetime import datetime, timedelta
        
        vault_client = get_vault_client()
        secret_data = vault_client.get_secret("jwt")
        
        if not secret_data or 'secret_key' not in secret_data:
            raise Exception("JWT secret not found in Vault")
        
        # üîç –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: –ª–æ–≥–∏—Ä—É–µ–º —Å–æ–∑–¥–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–∞
        logger.debug(f"üîç DIAGNOSTIC: Creating JWT token for user_id={user_id}")
        
        # –°–æ–∑–¥–∞–µ–º —Ç–æ–∫–µ–Ω –¥–ª—è invite-service —Å —Ä–µ–∞–ª—å–Ω—ã–º user_id
        payload = {
            'service': 'invite-service',
            'user_id': user_id,  # ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–û: –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–∞–ª—å–Ω—ã–π user_id
            'exp': int((datetime.utcnow() + timedelta(hours=1)).timestamp())
        }
        
        import jwt
        token = jwt.encode(payload, secret_data['secret_key'], algorithm='HS256')
        
        logger.debug(f"üîç DIAGNOSTIC: JWT token created successfully for user_id={user_id}")
        return token
        
    except Exception as e:
        logger.error(f"Error getting JWT token for parsing service: {e}")
        raise 