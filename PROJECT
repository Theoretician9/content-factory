# PROJECT — Полное описание проекта

> **Этот файл — основное описание проекта. Здесь всегда хранится актуальное, максимально подробное описание архитектуры, сервисов, инфраструктуры, статуса и принципов работы. Любой новый участник может ознакомиться с этим файлом и сразу понять, как устроен проект, что реализовано, а что ещё нет. Все изменения должны своевременно отражаться здесь.**

## Назначение
SaaS-платформа для автоматизации маркетинга, генерации контента и мультиканальных воронок.

## Архитектура
- Микросервисная архитектура
- Каждый сервис — отдельный контейнер
- Взаимодействие через API Gateway и backend-сеть
- Все сервисы и инфраструктурные компоненты работают в Docker Compose

## Технологии
- Python (FastAPI)
- MySQL
- Redis
- RabbitMQ
- Nginx
- Prometheus
- Grafana
- Vault
- Elasticsearch
- Kibana
- Logstash
- Docker Compose

## Инфраструктура
- ✅ Docker Compose настроен и работает
- ✅ Единая сеть backend для всех сервисов
- ✅ Volume для хранения данных (MySQL, Redis, RabbitMQ, Elasticsearch, Vault)
- ✅ Все базовые сервисы (MySQL, Redis, RabbitMQ, Vault, Prometheus, Grafana, ELK) работают и доступны
- ✅ Все volumes и сети корректно проброшены
- ✅ Нет лишних открытых портов наружу
- ⚠️ Админские сервисы (Grafana, Prometheus, Kibana, Alertmanager, Vault) доступны только из внутренней docker-сети или через SSH-туннель. Наружу открыты только 80 и 443 для публичных сервисов (API Gateway, фронт).

## База данных
- ✅ MySQL запущен и доступен на порту 3307
- ✅ Создана база данных integration_service
- ✅ Настроен пользователь telegraminvi с необходимыми правами
- ❌ Остальные базы данных для сервисов ещё не созданы
- ❌ Не настроены миграции и схемы баз данных

## ELK Stack (Elasticsearch, Logstash, Kibana)
- ✅ Elasticsearch запущен, доступен на порту 9200, индексы создаются корректно
- ✅ Logstash запущен, читает логи из /var/www/html/logs/services/ и /var/www/html/logs/api-gateway/ (volume проброшен абсолютным путём)
- ✅ Тестовые логи обработаны, индекс logs-YYYY.MM.DD появляется в Elasticsearch
- ✅ Документы содержат поля: @timestamp, level, message, type и др.
- ⚠️ Kibana: index pattern logs-* создаётся, но поле Timestamp field неактивно (вечная загрузка) — будет решено после появления реальных логов
- ⚠️ Визуализации и дашборды будут настраиваться после поступления реальных логов

## Мониторинг
- ✅ Prometheus запущен (только во внутренней сети)
- ✅ Grafana запущена (только во внутренней сети)
- ✅ Grafana и Prometheus находятся в одной docker-сети, интеграция подтверждена
- ✅ Grafana успешно видит Prometheus по адресу http://prometheus:9090
- ✅ Метрики с MySQL, RabbitMQ, Redis, а также с backend-сервисов собираются (экспортеры работают)
- ⚠️ Для доступа к Grafana, Prometheus, Kibana, Alertmanager используйте SSH-туннель
- ❌ Не настроены пользовательские дашборды в Grafana (можно создавать)

## Сервисы
- ✅ API Gateway запущен и доступен на порту 8000
- ✅ Integration Service запущен
- ❌ Остальные микросервисы (User, Billing, Scenario, Content, Invite, Parsing) ещё не протестированы
- ❌ Не настроено взаимодействие между сервисами
- ❌ Не протестированы API эндпоинты

## Безопасность
- ✅ Vault запущен (только во внутренней сети)
- ✅ Настроен root token для Vault
- ✅ Проброс портов для админских сервисов убран, доступ только через внутреннюю сеть или SSH-туннель
- ❌ Не настроено хранение секретов в Vault
- ❌ Не настроен HTTPS
- ❌ Не настроен firewall

## Очереди и кэширование
- ✅ Redis запущен и доступен на порту 6380
- ✅ RabbitMQ запущен и доступен на портах 5672, 15672, 15692
- ❌ Не настроены очереди в RabbitMQ
- ❌ Не настроено кэширование в Redis

## Доступы
- API Gateway: http://92.113.146.148:8000 (будет http/https, публично)
- Grafana: только через SSH-туннель (локально http://localhost:3000)
- Kibana: только через SSH-туннель (локально http://localhost:5601)
- Prometheus: только через SSH-туннель (локально http://localhost:9090)
- Alertmanager: только через SSH-туннель (локально http://localhost:9093)
- RabbitMQ: http://92.113.146.148:15672 (user/password)
- MySQL root: root/Lfnm97HnPug8
- MySQL user: telegraminvi/szkTgBhWh6XU
- Vault: только через SSH-туннель (локально http://localhost:8201, root token)

### Безопасность и доступ к админским сервисам
- Все админские сервисы (Grafana, Prometheus, Alertmanager, Kibana, Vault, RabbitMQ Management, Logstash Monitoring, Elasticsearch) проброшены только на 127.0.0.1 сервера.
- Доступ к ним возможен только через SSH-туннель.
- Наружу открыты только порты 80 и 443.
- Админские интерфейсы полностью изолированы от внешнего мира.

## Внешний reverse proxy (nginx)
- Для публикации приложения на домене content-factory.xyz используется отдельный сервис nginx в docker-compose.
- Nginx слушает порты 80 и 443, проксирует все запросы на сервис api-gateway (порт 8000).
- Конфиг nginx поддерживает работу Let's Encrypt (автоматический выпуск HTTPS-сертификатов).
- Вся внешняя маршрутизация и SSL-терминация происходит на уровне этого nginx.
- Внутри контейнера api-gateway также используется nginx, но только для внутренних нужд (например, проксирование к integration-service).
- Внешний доступ к приложению осуществляется только через nginx по адресу http://content-factory.xyz (и в будущем https://content-factory.xyz).

### HTTPS и безопасность
- Домен content-factory.xyz обслуживается через nginx с поддержкой HTTPS (Let's Encrypt).
- Сертификаты выпускаются и продлеваются с помощью certbot (docker-compose).
- Для автоматического продления сертификата используется команда: docker-compose run --rm certbot renew && docker-compose restart nginx (рекомендуется добавить в cron).
- nginx автоматически редиректит все http-запросы на https.
- Вся внешняя маршрутизация и SSL-терминация происходит на уровне nginx.

### Централизованное хранение секретов (Vault)
- Все чувствительные данные (пароли, токены, ключи, CSRF/JWT secret) выносятся в HashiCorp Vault.
- API Gateway получает CSRF_SECRET_KEY и JWT_SECRET_KEY из Vault, .env не содержит секретов в открытом виде.
- Интеграция с Vault реализована для API Gateway, Integration Service и постепенно внедряется для остальных сервисов.
- Структура хранения: secret/<service>/<ключ> (например, secret/mysql/root_password, secret/user-service/secret_key).
- Пример записи секрета:
  vault kv put secret/mysql root_password=Lfnm97HnPug8 user=telegraminvi user_password=szkTgBhWh6XU
- Пример получения секрета в Python:
  import hvac
  client = hvac.Client(url='http://vault:8201', token='root')
  secret = client.secrets.kv.v2.read_secret_version(path='mysql')
  print(secret['data']['data']['root_password'])
- Постепенно все сервисы будут получать секреты из Vault, а не из .env/docker-compose.
- В docker-compose и .env останутся только VAULT_ADDR и VAULT_TOKEN.

### Планы по фронтенду
1. Первый фронт — админка:
   - Мониторинг пользователей (список, фильтры, поиск)
   - Просмотр расходов, оплат, баланса по пользователям
   - Управление пользователями (блокировка, лимиты)
   - Дашборды по метрикам (интеграция с Grafana через iframe или API)
   - Просмотр логов (интеграция с Kibana или отдельный UI)
   - Управление тарифами, настройками, интеграциями
   - Доступ только по https, авторизация только для админов (JWT/OAuth2)
   - Все чувствительные данные — только через Vault
   - Логи действий админов (audit trail)
2. Второй фронт — пользовательский аккаунт:
   - Регистрация пользователя (email, телефон, соцсети)
   - Вход/выход, восстановление пароля
   - Личный кабинет: профиль, тариф, история оплат, настройки интеграций
   - Просмотр и управление своими расходами, лимитами, балансом
   - Интеграция с backend API (через API Gateway)
   - HTTPS, защита от CSRF, rate limiting, 2FA (по желанию)
   - **Лендинг реализован, все секции (описание, преимущества, тарифы, FAQ, форма обратной связи) и SEO-теги работают, фронт деплоится через volume frontend-static и nginx.**
   - **Лендинг полностью адаптивен, протестирован на всех устройствах, нет горизонтального скролла, визуал и UX доработаны для мобильных и планшетов.**

#### Архитектурные рекомендации
- Фронтенды как отдельные docker-сервисы (SPA: React/Vue/Svelte)
- API Gateway — единственная точка входа для всех фронтов
- JWT-авторизация (разные роли: admin, user), секреты для подписи токенов — только через Vault
- Мониторинг: интеграция с Grafana/Kibana через iframe или API (для админки)
- Документация API: Swagger/OpenAPI, доступен только авторизованным
- Все секреты — только через Vault, HTTPS для всех фронтов, логи действий

- Swagger UI и ReDoc отключены во внешней среде (docs_url, redoc_url = None), доступны только при DEBUG=true.
- OpenAPI JSON (/openapi.json) остаётся доступен для интеграций.
- Логирование (audit trail) теперь реализовано для login, logout, register, ошибок аутентификации. Все логи в формате JSON для Logstash/ELK.
- Security схемы (JWT, CSRF) описаны в OpenAPI/Swagger.

- Для /auth/login и /auth/register реализована строгая валидация входных данных по pydantic-схемам, ошибки валидации логируются, OpenAPI обновляется автоматически.

- Volume frontend-static для фронта теперь не анонимный, а локальная папка (./frontend-static:/usr/share/nginx/html:ro). Всё содержимое этой папки автоматически доступно nginx и на https://content-factory.xyz/.

- ✅ Интеграция auth (регистрация и логин) через API Gateway завершена, все критические ошибки устранены
- ✅ После регистрации на фронте автоматически выполняется логин, токены сохраняются, происходит редирект на /dashboard
- ✅ Регистрация и автоматический логин работают через api-gateway, интеграция auth завершена
- ⏳ Следующий шаг: добавить капчу на регистрацию и email-валидацию

### Пользовательский кабинет (Dashboard)
- Sidebar (левое меню) реализован как отдельный компонент
- На desktop Sidebar всегда открыт, на мобильных скрывается и открывается по кнопке-гамбургеру в Header
- Для мобильных реализован overlay (затемнение фона) и крестик для закрытия меню
- После выбора пункта меню Sidebar автоматически закрывается на мобильных
- Используются Tailwind, SVG-иконки, поддержка светлой/тёмной темы
- Вся навигация и адаптивность реализованы через состояние React и Tailwind-классы, без прямого обращения к window.innerWidth в JSX
- **В текущей версии проекта роли пользователей не реализованы. После внедрения ролей потребуется реализовать и протестировать отображение Dashboard для разных ролей.**

---

## Текущие сервисы и контейнеры

### 1. Prometheus
- **Назначение:** Сбор метрик со всех сервисов, инфраструктуры, экспортёров.
- **Внутренний адрес:** prometheus:9090 (docker-сеть backend)
- **Внешний адрес:** Только через SSH-туннель: http://localhost:9090
- **Данные:** ./prometheus:/etc/prometheus (volume)
- **Команды:**
  - Перезапуск: `docker-compose restart prometheus`
  - Логи: `docker-compose logs prometheus --tail 100`
  - Войти в контейнер: `docker-compose exec prometheus sh`
- **Особенности:**
  - Конфиг prometheus.yml настраивает источники метрик
  - Метрики доступны только из внутренней сети

### 2. Grafana
- **Назначение:** Визуализация метрик, дашборды, алерты
- **Внутренний адрес:** grafana:3000
- **Внешний адрес:** Только через SSH-туннель: http://localhost:3000
- **Данные:** ./grafana:/var/lib/grafana (volume)
- **Команды:**
  - Перезапуск: `docker-compose restart grafana`
  - Логи: `docker-compose logs grafana --tail 100`
  - Войти в контейнер: `docker-compose exec grafana sh`
- **Особенности:**
  - Дефолтный логин: admin/admin (сменить после первого входа)
  - Дашборды и источники данных сохраняются в volume

### 3. Kibana
- **Назначение:** Визуализация логов из Elasticsearch
- **Внутренний адрес:** kibana:5601
- **Внешний адрес:** Только через SSH-туннель: http://localhost:5601
- **Команды:**
  - Перезапуск: `docker-compose restart kibana`
  - Логи: `docker-compose logs kibana --tail 100`
  - Войти в контейнер: `docker-compose exec kibana sh`
- **Особенности:**
  - Требует индекс logs-* в Elasticsearch

### 4. Elasticsearch
- **Назначение:** Хранение логов, поиск, аналитика
- **Внутренний адрес:** elasticsearch:9200
- **Внешний адрес:** Только через SSH-туннель: http://localhost:9200
- **Данные:** ./elasticsearch:/usr/share/elasticsearch/data (volume)
- **Команды:**
  - Перезапуск: `docker-compose restart elasticsearch`
  - Логи: `docker-compose logs elasticsearch --tail 100`
  - Войти в контейнер: `docker-compose exec elasticsearch sh`
- **Особенности:**
  - Требует много памяти (рекомендуется 2ГБ+)

### 5. Logstash
- **Назначение:** Парсинг и обработка логов, отправка в Elasticsearch
- **Внутренний адрес:** logstash:5044
- **Данные:** ./logstash:/usr/share/logstash/pipeline (volume)
- **Команды:**
  - Перезапуск: `docker-compose restart logstash`
  - Логи: `docker-compose logs logstash --tail 100`
  - Войти в контейнер: `docker-compose exec logstash sh`
- **Особенности:**
  - Конфиг logstash.conf настраивает парсинг логов

### 6. Alertmanager
- **Назначение:** Алерты по метрикам Prometheus
- **Внутренний адрес:** alertmanager:9093
- **Внешний адрес:** Только через SSH-туннель: http://localhost:9093
- **Команды:**
  - Перезапуск: `docker-compose restart alertmanager`
  - Логи: `docker-compose logs alertmanager --tail 100`
  - Войти в контейнер: `docker-compose exec alertmanager sh`

### 7. MySQL
- **Назначение:** Основная база данных для сервисов
- **Внутренний адрес:** mysql:3306
- **Внешний адрес:** Только через SSH-туннель: localhost:3307
- **Данные:** ./mysql:/var/lib/mysql (volume)
- **Пользователи:**
  - root/Lfnm97HnPug8
  - telegraminvi/szkTgBhWh6XU
- **Команды:**
  - Перезапуск: `docker-compose restart mysql`
  - Логи: `docker-compose logs mysql --tail 100`
  - Войти в контейнер: `docker-compose exec mysql bash`
  - MySQL shell: `docker-compose exec mysql mysql -u root -p`

### 8. Redis
- **Назначение:** Кэш, очереди, хранение refresh token
- **Внутренний адрес:** redis:6379
- **Внешний адрес:** Только через SSH-туннель: localhost:6380
- **Данные:** ./redis:/data (volume)
- **Команды:**
  - Перезапуск: `docker-compose restart redis`
  - Логи: `docker-compose logs redis --tail 100`
  - Войти в контейнер: `docker-compose exec redis sh`
  - Redis CLI: `docker-compose exec redis redis-cli`

### 9. RabbitMQ
- **Назначение:** Очереди задач между сервисами
- **Внутренний адрес:** rabbitmq:5672
- **Внешний адрес:** Только через SSH-туннель: http://localhost:15672 (UI)
- **Данные:** ./rabbitmq:/var/lib/rabbitmq (volume)
- **Команды:**
  - Перезапуск: `docker-compose restart rabbitmq`
  - Логи: `docker-compose logs rabbitmq --tail 100`
  - Войти в контейнер: `docker-compose exec rabbitmq bash`
- **Особенности:**
  - UI: http://localhost:15672 (user/password)

### 10. Vault
- **Назначение:** Хранение секретов, токенов, ключей
- **Внутренний адрес:** vault:8201
- **Внешний адрес:** Только через SSH-туннель: http://localhost:8201
- **Данные:** ./vault:/vault/file (volume)
- **Команды:**
  - Перезапуск: `docker-compose restart vault`
  - Логи: `docker-compose logs vault --tail 100`
  - Войти в контейнер: `docker-compose exec vault sh`
- **Особенности:**
  - root token: root
  - Все секреты для сервисов хранятся здесь

### 11. API Gateway (api-gateway)
- **Назначение:** Единая точка входа для всех фронтов и сервисов, проксирование, аутентификация, лимиты, CSRF, логирование
- **Внутренний адрес:** api-gateway:8000
- **Внешний адрес:** http://92.113.146.148:8000 (или через nginx)
- **Команды:**
  - Перезапуск: `docker-compose restart api-gateway`
  - Логи: `docker-compose logs api-gateway --tail 100`
  - Войти в контейнер: `docker-compose exec api-gateway sh`
- **Особенности:**
  - Все запросы с фронта идут через этот сервис
  - Лимиты через slowapi, логирование в stdout (JSON)
  - Секреты (CSRF, JWT) берутся из Vault

### 12. User Service (user-service)
- **Назначение:** Регистрация, логин, управление пользователями
- **Внутренний адрес:** user-service:8001
- **Команды:**
  - Перезапуск: `docker-compose restart user-service`
  - Логи: `docker-compose logs user-service --tail 100`
  - Войти в контейнер: `docker-compose exec user-service sh`
- **Особенности:**
  - Работает только через api-gateway
  - Подключается к MySQL (user_service)

### 13. Integration Service (integration-service)
- **Назначение:** Пример микросервиса для интеграций
- **Внутренний адрес:** integration-service:8000
- **Команды:**
  - Перезапуск: `docker-compose restart integration-service`
  - Логи: `docker-compose logs integration-service --tail 100`
  - Войти в контейнер: `docker-compose exec integration-service sh`

### 14. Frontend
- **Назначение:** Внешний SPA (React)
- **Внутренний адрес:** frontend:3000 (dev), статика деплоится в ./frontend-static
- **Внешний адрес:** https://content-factory.xyz
- **Данные:** ./frontend-static:/usr/share/nginx/html:ro (volume)
- **Команды:**
  - Сборка: `cd frontend && npm run build`
  - Перезапуск nginx: `docker-compose restart nginx`
- **Особенности:**
  - Все статика деплоится через volume
  - Для разработки: `npm run dev` (порт 3000)

### 15. Nginx (reverse proxy)
- **Назначение:** Внешний reverse proxy, SSL-терминация, публикация фронта и API
- **Внутренний адрес:** nginx:80, nginx:443
- **Внешний адрес:** http://content-factory.xyz, https://content-factory.xyz
- **Данные:** ./nginx.conf, ./frontend-static
- **Команды:**
  - Перезапуск: `docker-compose restart nginx`
  - Логи: `docker-compose logs nginx --tail 100`
  - Войти в контейнер: `docker-compose exec nginx sh`
- **Особенности:**
  - SSL через Let's Encrypt
  - Проксирует все запросы на api-gateway и frontend

---
> **В этом файле не ведётся хронология изменений! Только актуальное состояние и описание. Для истории изменений используйте project-status.md** 