# PROJECT — Полное описание проекта

> **Этот файл — основное описание проекта. Здесь всегда хранится актуальное, максимально подробное описание архитектуры, сервисов, инфраструктуры, статуса и принципов работы. Любой новый участник может ознакомиться с этим файлом и сразу понять, как устроен проект, что реализовано, а что ещё нет. Все изменения должны своевременно отражаться здесь.**

## Назначение
SaaS-платформа для автоматизации маркетинга, генерации контента и мультиканальных воронок.

## Архитектура
- Микросервисная архитектура
- Каждый сервис — отдельный контейнер
- Взаимодействие через API Gateway и backend-сеть
- Все сервисы и инфраструктурные компоненты работают в Docker Compose

## Технологии
- Python (FastAPI)
- MySQL
- Redis
- RabbitMQ
- Nginx
- Prometheus
- Grafana
- Vault
- Elasticsearch
- Kibana
- Logstash
- Docker Compose

## Инфраструктура
- ✅ Docker Compose настроен и работает
- ✅ Единая сеть backend для всех сервисов
- ✅ Volume для хранения данных (MySQL, Redis, RabbitMQ, Elasticsearch, Vault)
- ✅ Все базовые сервисы (MySQL, Redis, RabbitMQ, Vault, Prometheus, Grafana, ELK) работают и доступны
- ✅ Все volumes и сети корректно проброшены
- ✅ Нет лишних открытых портов наружу
- ⚠️ Админские сервисы (Grafana, Prometheus, Kibana, Alertmanager, Vault) доступны только из внутренней docker-сети или через SSH-туннель. Наружу открыты только 80 и 443 для публичных сервисов (API Gateway, фронт).

## База данных
- ✅ MySQL запущен и доступен на порту 3307
- ✅ Создана база данных integration_service
- ✅ Настроен пользователь telegraminvi с необходимыми правами
- ❌ Остальные базы данных для сервисов ещё не созданы
- ❌ Не настроены миграции и схемы баз данных

## ELK Stack (Elasticsearch, Logstash, Kibana)
- ✅ Elasticsearch запущен, доступен на порту 9200, индексы создаются корректно
- ✅ Logstash запущен, читает логи из /var/www/html/logs/services/ и /var/www/html/logs/api-gateway/ (volume проброшен абсолютным путём)
- ✅ Тестовые логи обработаны, индекс logs-YYYY.MM.DD появляется в Elasticsearch
- ✅ Документы содержат поля: @timestamp, level, message, type и др.
- ⚠️ Kibana: index pattern logs-* создаётся, но поле Timestamp field неактивно (вечная загрузка) — будет решено после появления реальных логов
- ⚠️ Визуализации и дашборды будут настраиваться после поступления реальных логов

## Мониторинг
- ✅ Prometheus запущен (только во внутренней сети)
- ✅ Grafana запущена (только во внутренней сети)
- ✅ Grafana и Prometheus находятся в одной docker-сети, интеграция подтверждена
- ✅ Grafana успешно видит Prometheus по адресу http://prometheus:9090
- ✅ Метрики с MySQL, RabbitMQ, Redis, а также с backend-сервисов собираются (экспортеры работают)
- ⚠️ Для доступа к Grafana, Prometheus, Kibana, Alertmanager используйте SSH-туннель
- ❌ Не настроены пользовательские дашборды в Grafana (можно создавать)

## Сервисы
- ✅ API Gateway запущен и доступен на порту 8000
- ✅ Integration Service запущен
- ❌ Остальные микросервисы (User, Billing, Scenario, Content, Invite, Parsing) ещё не протестированы
- ❌ Не настроено взаимодействие между сервисами
- ❌ Не протестированы API эндпоинты

## Безопасность
- ✅ Vault запущен (только во внутренней сети)
- ✅ Настроен root token для Vault
- ✅ Проброс портов для админских сервисов убран, доступ только через внутреннюю сеть или SSH-туннель
- ❌ Не настроено хранение секретов в Vault
- ❌ Не настроен HTTPS
- ❌ Не настроен firewall

## Очереди и кэширование
- ✅ Redis запущен и доступен на порту 6380
- ✅ RabbitMQ запущен и доступен на портах 5672, 15672, 15692
- ❌ Не настроены очереди в RabbitMQ
- ❌ Не настроено кэширование в Redis

## Доступы
- API Gateway: http://92.113.146.148:8000 (будет http/https, публично)
- Grafana: только через SSH-туннель (локально http://localhost:3000)
- Kibana: только через SSH-туннель (локально http://localhost:5601)
- Prometheus: только через SSH-туннель (локально http://localhost:9090)
- Alertmanager: только через SSH-туннель (локально http://localhost:9093)
- RabbitMQ: http://92.113.146.148:15672 (user/password)
- MySQL root: root/Lfnm97HnPug8
- MySQL user: telegraminvi/szkTgBhWh6XU
- Vault: только через SSH-туннель (локально http://localhost:8201, root token)

### Безопасность и доступ к админским сервисам
- Все админские сервисы (Grafana, Prometheus, Alertmanager, Kibana, Vault, RabbitMQ Management, Logstash Monitoring, Elasticsearch) проброшены только на 127.0.0.1 сервера.
- Доступ к ним возможен только через SSH-туннель.
- Наружу открыты только порты 80 и 443.
- Админские интерфейсы полностью изолированы от внешнего мира.

## Внешний reverse proxy (nginx)
- Для публикации приложения на домене content-factory.xyz используется отдельный сервис nginx в docker-compose.
- Nginx слушает порты 80 и 443, проксирует все запросы на сервис api-gateway (порт 8000).
- Конфиг nginx поддерживает работу Let's Encrypt (автоматический выпуск HTTPS-сертификатов).
- Вся внешняя маршрутизация и SSL-терминация происходит на уровне этого nginx.
- Внутри контейнера api-gateway также используется nginx, но только для внутренних нужд (например, проксирование к integration-service).
- Внешний доступ к приложению осуществляется только через nginx по адресу http://content-factory.xyz (и в будущем https://content-factory.xyz).

### HTTPS и безопасность
- Домен content-factory.xyz обслуживается через nginx с поддержкой HTTPS (Let's Encrypt).
- Сертификаты выпускаются и продлеваются с помощью certbot (docker-compose).
- Для автоматического продления сертификата используется команда: docker-compose run --rm certbot renew && docker-compose restart nginx (рекомендуется добавить в cron).
- nginx автоматически редиректит все http-запросы на https.
- Вся внешняя маршрутизация и SSL-терминация происходит на уровне nginx.

### Централизованное хранение секретов (Vault)
- Все чувствительные данные (пароли, токены, ключи, CSRF/JWT secret) выносятся в HashiCorp Vault.
- API Gateway получает CSRF_SECRET_KEY и JWT_SECRET_KEY из Vault, .env не содержит секретов в открытом виде.
- Интеграция с Vault реализована для API Gateway, Integration Service и постепенно внедряется для остальных сервисов.
- Структура хранения: secret/<service>/<ключ> (например, secret/mysql/root_password, secret/user-service/secret_key).
- Пример записи секрета:
  vault kv put secret/mysql root_password=Lfnm97HnPug8 user=telegraminvi user_password=szkTgBhWh6XU
- Пример получения секрета в Python:
  import hvac
  client = hvac.Client(url='http://vault:8201', token='root')
  secret = client.secrets.kv.v2.read_secret_version(path='mysql')
  print(secret['data']['data']['root_password'])
- Постепенно все сервисы будут получать секреты из Vault, а не из .env/docker-compose.
- В docker-compose и .env останутся только VAULT_ADDR и VAULT_TOKEN.

### Планы по фронтенду
1. Первый фронт — админка:
   - Мониторинг пользователей (список, фильтры, поиск)
   - Просмотр расходов, оплат, баланса по пользователям
   - Управление пользователями (блокировка, лимиты)
   - Дашборды по метрикам (интеграция с Grafana через iframe или API)
   - Просмотр логов (интеграция с Kibana или отдельный UI)
   - Управление тарифами, настройками, интеграциями
   - Доступ только по https, авторизация только для админов (JWT/OAuth2)
   - Все чувствительные данные — только через Vault
   - Логи действий админов (audit trail)
2. Второй фронт — пользовательский аккаунт:
   - Регистрация пользователя (email, телефон, соцсети)
   - Вход/выход, восстановление пароля
   - Личный кабинет: профиль, тариф, история оплат, настройки интеграций
   - Просмотр и управление своими расходами, лимитами, балансом
   - Интеграция с backend API (через API Gateway)
   - HTTPS, защита от CSRF, rate limiting, 2FA (по желанию)
   - **Лендинг реализован, все секции (описание, преимущества, тарифы, FAQ, форма обратной связи) и SEO-теги работают, фронт деплоится через volume frontend-static и nginx.**
   - **Лендинг полностью адаптивен, протестирован на всех устройствах, нет горизонтального скролла, визуал и UX доработаны для мобильных и планшетов.**

#### Архитектурные рекомендации
- Фронтенды как отдельные docker-сервисы (SPA: React/Vue/Svelte)
- API Gateway — единственная точка входа для всех фронтов
- JWT-авторизация (разные роли: admin, user), секреты для подписи токенов — только через Vault
- Мониторинг: интеграция с Grafana/Kibana через iframe или API (для админки)
- Документация API: Swagger/OpenAPI, доступен только авторизованным
- Все секреты — только через Vault, HTTPS для всех фронтов, логи действий

- Swagger UI и ReDoc отключены во внешней среде (docs_url, redoc_url = None), доступны только при DEBUG=true.
- OpenAPI JSON (/openapi.json) остаётся доступен для интеграций.
- Логирование (audit trail) теперь реализовано для login, logout, register, ошибок аутентификации. Все логи в формате JSON для Logstash/ELK.
- Security схемы (JWT, CSRF) описаны в OpenAPI/Swagger.

- Для /auth/login и /auth/register реализована строгая валидация входных данных по pydantic-схемам, ошибки валидации логируются, OpenAPI обновляется автоматически.

- Volume frontend-static для фронта теперь не анонимный, а локальная папка (./frontend-static:/usr/share/nginx/html:ro). Всё содержимое этой папки автоматически доступно nginx и на https://content-factory.xyz/.

---
> **В этом файле не ведётся хронология изменений! Только актуальное состояние и описание. Для истории изменений используйте project-status.md** 